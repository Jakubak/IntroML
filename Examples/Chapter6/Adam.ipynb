{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create loss and gradient functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(W):\n",
    "    return 2*W[0]*W[0] + W[1]*W[1]\n",
    "\n",
    "def gradient(W):\n",
    "    return np.array([4*W[0],2*W[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ADAM\")\n",
    "W0 = np.array([2,2])\n",
    "alpha = 0.1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "eps=0\n",
    "print(\"W0: {}\".format(W0))\n",
    "print(\"Loss W0: {}\".format(loss(W0)))\n",
    "v0 = 0\n",
    "m0 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Rule\n",
    "$Update_{epoch=i}=-\\frac{\\alpha}{\\sqrt{v_{epoch=i}} + \\epsilon}m_{epoch=i}$\n",
    "\n",
    "where, \n",
    "\n",
    "+ $m_{epoch=i} = \\beta_1m_{epoch=i-1} + (1-\\beta_1) \\nabla_WL_{epoch=i-1}, \\quad m_{epoch=0} = 0$\n",
    "\n",
    "+ $v_{epoch=i} = \\beta_2v_{epoch=i-1} + (1-\\beta_2) \\nabla_WL^2_{epoch=i-1}, \\quad v_{epoch=0} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1\n",
    "gradW0 = gradient(W0)\n",
    "grad2W0 = np.square(gradW0)\n",
    "print(\"Gradient W0: {}\".format(gradW0))\n",
    "print(\"Gradient2 W0: {}\".format(grad2W0))\n",
    "m1 = beta1*m0 + (1-beta1)*gradient(W0)\n",
    "v1 = beta2*v0 + (1-beta2)*np.square(gradient(W0))\n",
    "W1 = W0 - alpha*m1/(np.sqrt(v1)+eps)\n",
    "print(\"m1: {}\".format(m1))\n",
    "print(\"v1: {}\".format(v1))\n",
    "print(\"W1: {}\".format(W1))\n",
    "print(\"Loss W1: {}\".format(loss(W1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 2\n",
    "gradW1 = gradient(W1)\n",
    "grad2W1 = np.square(gradW1)\n",
    "print(\"Gradient W1: {}\".format(gradW1))\n",
    "print(\"Gradient2 W1: {}\".format(grad2W1))\n",
    "m2 = beta1*m1 + (1-beta1)*gradient(W1)\n",
    "v2 = beta2*v1 + (1-beta2)*np.square(gradient(W1))\n",
    "W2 = W1 - alpha*m1/(np.sqrt(v2)+eps)\n",
    "print(\"m2: {}\".format(m2))\n",
    "print(\"v2: {}\".format(v2))\n",
    "print(\"W2: {}\".format(W2))\n",
    "print(\"Loss W2: {}\".format(loss(W2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntroML-env",
   "language": "python",
   "name": "introml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
