{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z: \n",
      "[[ 0.1 -0.1 -0.2]\n",
      " [-0.2  0.2  0.3]\n",
      " [-0.3  0.1  0.2]\n",
      " [ 0.4 -0.3 -0.5]]\n",
      "A: \n",
      "[[0.26588694 0.22779083 0.20486076]\n",
      " [0.19697389 0.30748546 0.3377583 ]\n",
      " [0.17822935 0.27822435 0.30561635]\n",
      " [0.35890983 0.18649936 0.15176459]]\n",
      "Sum of A in row direction: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Softmax Example\n",
    "Z = np.array([[0.1,-0.1,-0.2],[-0.2, 0.2, 0.3],[-0.3,0.1,0.2],[0.4,-0.3,-0.5]])\n",
    "Zexp = np.exp(Z)\n",
    "Sum = np.sum(Zexp,axis=0,keepdims=True)\n",
    "A = Zexp/Sum\n",
    "print(\"Z: \\n{}\".format(Z))\n",
    "print(\"A: \\n{}\".format(A))\n",
    "# sum in row direction (down each column)\n",
    "print(\"Sum of A in row direction: {}\".format(np.sum(A,axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation\n",
    "#training data\n",
    "X = np.array([[1,2,4],[-2,-5,-8]])\n",
    "Y = np.array([[0,1,2]])\n",
    "# parameters\n",
    "W1 = np.array([[0.5,0.5],[0.5,-0.5]])\n",
    "b1 = np.array([[0.5],[0.5]])\n",
    "W2 = np.array([[-1,1],[1,-1],[-2,1]])\n",
    "b2 = np.array([[-0.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1: \n",
      "[[ 0.  -1.  -1.5]\n",
      " [ 2.   4.   6.5]]\n",
      "A1: \n",
      "[[ 0.         -0.76159416 -0.90514825]\n",
      " [ 0.96402758  0.9993293   0.99999548]]\n",
      "Z2: \n",
      "[[ 0.86402758  1.66092346  1.80514373]\n",
      " [-1.06402758 -1.86092346 -2.00514373]\n",
      " [ 0.86402758  2.42251761  2.71029199]]\n",
      "A2: \n",
      "[[0.46610686 0.31533481 0.28616887]\n",
      " [0.06778628 0.00931651 0.0063363 ]\n",
      " [0.46610686 0.67534868 0.70749484]]\n"
     ]
    }
   ],
   "source": [
    "# layer 1\n",
    "Z1 = np.dot(W1,X) + b1\n",
    "print(\"Z1: \\n{}\".format(Z1))\n",
    "A1 = np.tanh(Z1)\n",
    "print(\"A1: \\n{}\".format(A1))\n",
    "# layer 2\n",
    "Z2 = np.dot(W2,A1) + b2\n",
    "print(\"Z2: \\n{}\".format(Z2))\n",
    "Z2exp = np.exp(Z2)\n",
    "Sum = np.sum(Z2exp,axis=0,keepdims=True)\n",
    "A2 = Z2exp/Sum\n",
    "print(\"A2: \\n{}\".format(A2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(Y,nclass):\n",
    "    ndata = Y.shape[1]\n",
    "    Y_onehot = np.zeros((nclass,ndata))\n",
    "    for count in range(ndata):\n",
    "        Y_onehot[int(Y[0,count]),count] = 1.0\n",
    "    return Y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACK PROPAGATION\n",
      "Yonehot: \n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "dLossdA2: [[ -0.71514359  -0.          -0.        ]\n",
      " [ -0.         -35.77877846  -0.        ]\n",
      " [ -0.          -0.          -0.47114596]]\n"
     ]
    }
   ],
   "source": [
    "print(\"BACK PROPAGATION\")\n",
    "# dLoss/dA2\n",
    "Yonehot = onehot(Y,3)\n",
    "print(\"Yonehot: \\n{}\".format(Yonehot))\n",
    "dLossdA2 = -Yonehot/A2/3\n",
    "print(\"dLossdA2: {}\".format(dLossdA2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2*dLossdA: [[-0.33333333 -0.         -0.        ]\n",
      " [-0.         -0.33333333 -0.        ]\n",
      " [-0.         -0.         -0.33333333]]\n",
      "sumterm: [-0.33333333 -0.33333333 -0.33333333]\n",
      "sumprod: [[-0.15536895 -0.1051116  -0.09538962]\n",
      " [-0.02259543 -0.0031055  -0.0021121 ]\n",
      " [-0.15536895 -0.22511623 -0.23583161]]\n",
      "dLossdZ2: [[-2.60914798e-01  3.31453475e-02  2.72975400e-02]\n",
      " [ 1.53165977e-03 -3.33304401e-01  1.33828888e-05]\n",
      " [ 7.24185355e-02  1.52031947e-01 -1.66483685e-01]]\n",
      "dLossdW2: [[-0.04995162 -0.19110853]\n",
      " [ 0.25383057 -0.33159091]\n",
      " [ 0.03490578  0.05526051]]\n",
      "dLossdb2: [-0.20047191 -0.33175936  0.0579668 ]\n",
      "dLossdA1: [[ 0.11760939 -0.67051364  0.30568321]\n",
      " [-0.19002792  0.51848169 -0.13919953]]\n"
     ]
    }
   ],
   "source": [
    "# LAYER 2\n",
    "# dLoss/dZ2\n",
    "prod2 = A2*dLossdA2\n",
    "print(\"A2*dLossdA: {}\".format(prod2))\n",
    "sumterm = np.sum(prod2,axis=0)\n",
    "print(\"sumterm: {}\".format(sumterm))\n",
    "sumprod = A2*sumterm\n",
    "print(\"sumprod: {}\".format(sumprod))\n",
    "dLossdZ2 = prod2 - A2*sumprod\n",
    "print(\"dLossdZ2: {}\".format(dLossdZ2))\n",
    "dLossdW2 = np.dot(dLossdZ2,A1.T)\n",
    "dLossdb2 = np.sum(dLossdZ2,axis=1)\n",
    "dLossdA1 = np.dot(W2.T,dLossdZ2)\n",
    "print(\"dLossdW2: {}\".format(dLossdW2))\n",
    "print(\"dLossdb2: {}\".format(dLossdb2))\n",
    "print(\"dLossdA1: {}\".format(dLossdA1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA1/dZ1: [[1.00000000e+00 4.19974342e-01 1.80706639e-01]\n",
      " [7.06508249e-02 1.34095068e-03 9.04127676e-06]]\n",
      "dLossdZ1: [[ 1.17609387e-01 -2.81598525e-01  5.52389862e-02]\n",
      " [-1.34256294e-02  6.95258383e-04 -1.25854146e-06]]\n",
      "dLossdW1: [[-0.22463172  0.73086196]\n",
      " [-0.01204015  0.02338504]]\n",
      "dLossdb1: [[-0.10875015]\n",
      " [-0.01273163]]\n"
     ]
    }
   ],
   "source": [
    "# LAYER 1\n",
    "dA1dZ1 = 1-A1*A1\n",
    "print(\"dA1/dZ1: {}\".format(dA1dZ1))\n",
    "dLossdZ1 = dLossdA1*dA1dZ1\n",
    "print(\"dLossdZ1: {}\".format(dLossdZ1))\n",
    "dLossdW1 = np.dot(dLossdZ1,X.T)\n",
    "dLossdb1 = np.sum(dLossdZ1,axis=1,keepdims=True)\n",
    "print(\"dLossdW1: {}\".format(dLossdW1))\n",
    "print(\"dLossdb1: {}\".format(dLossdb1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORWARD PROPAGATION\n",
      "Z1: [[ 0.  -1.  -1.5]\n",
      " [ 2.   4.   6.5]]\n",
      "A1: [[ 0.         -0.76159416 -0.90514825]\n",
      " [ 0.96402758  0.9993293   0.99999548]]\n",
      "Z2: [[ 0.86402758  1.66092346  1.80514373]\n",
      " [-1.06402758 -1.86092346 -2.00514373]\n",
      " [ 0.86402758  2.42251761  2.71029199]]\n",
      "A2: [[0.46610686 0.31533481 0.28616887]\n",
      " [0.06778628 0.00931651 0.0063363 ]\n",
      " [0.46610686 0.67534868 0.70749484]]\n",
      "P: [0 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Prediction Example\n",
    "print(\"FORWARD PROPAGATION\")\n",
    "# layer 1\n",
    "Z1 = np.dot(W1,X) + b1\n",
    "print(\"Z1: {}\".format(Z1))\n",
    "A1 = np.tanh(Z1)\n",
    "print(\"A1: {}\".format(A1))\n",
    "# layer 2\n",
    "Z2 = np.dot(W2,A1) + b2\n",
    "print(\"Z2: {}\".format(Z2))\n",
    "Z2exp = np.exp(Z2)\n",
    "A2 = Z2exp/np.sum(Z2exp,axis=0,keepdims=True)\n",
    "print(\"A2: {}\".format(A2))\n",
    "# prediction\n",
    "P = np.argmax(A2,axis=0)\n",
    "print(\"P: {}\".format(P))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse-env",
   "language": "python",
   "name": "mlcourse-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
